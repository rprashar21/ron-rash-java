what is caching ??
Caching is a temporary storage layer that stores frequently accessed or expensive-to-fetch data in a fast-access medium like RAM.
The main goals are:

Reduce latency
Decrease load on downstream systems (like databases or APIs)
Improve application throughput and scalability
Real-world use cases:
Netflix: Caching movie metadata or user watch history to avoid DB round-trips
Amazon: Caching product pages or stock availability
Google: Caching autocomplete results in search
Backend APIs: Caching authorization tokens or rate-limit counters



Q What if the redis or our cache  is down
If Redis is down, our cache layer is temporarily unavailable, which increases DB load and latency. To handle this gracefully, we:
Fallback to the DB for reads
Write to DB first and cache second to ensure persistence
Wrap cache calls in circuit breakers and use retries with backoff
Use Prometheus and Grafana to monitor cache health and alert SRE teams


Q Different type of cache
1.write thru cache -- data is written in db and cache synchronoulsy
cons - May slow down system under heavy write load. and hig latency, perforamnce is low
Use case - User profile updates, login sessions

2.write behind cache -- cache is updated and the db is updated asynchronously... low latency ,, good peromance
cons- if cache fails data might be lost ,hard to debug
use case -- logginsystems and analytical systems

3. Write-Around Cache -- db is wrtten first and then cache is updated on a read ,
prevents cache pollution
Use case: Bulk data inserts, infrequently accessed writes.

Q Which Would You Choose for a Financial Transaction System?
For a financial system like payments or fund transfers, I'd choose write-through caching to ensure t
hat every write is persisted to the database immediately.
 This guarantees data consistency and minimizes the risk of loss, even though it comes at a performance cost.

q in memory cache vs distributed cache
data is stored in aplication lvel , cannot survive restanrts,, fast as no netwrok callls
Caching configuration values, recent DB query results, or small reference data in a single-node app
eg linkedlist+hashmap

Distributed cache -- Examples: Redis, Memcached, Hazelcast
cale horizontal
data is shared across intsance , data can be restored survice app restart

Caching authentication tokens, user sessions, search results in microservices


Q Differernce between LRU LFU AND TTL
| Policy | Eviction Logic        | Used For                          |
| ------ | --------------------- | --------------------------------- |
| LRU    | Least Recently Used   | Web sessions, product views       |
| LFU    | Least Frequently Used | Caching most used data (e.g. DNS) |
| TTL    | Time-based Expiry     | OTPs, price cache, auth tokens    |

TTL -- expiry time with caching -- look at the springboot example and also in memory cache examples
Use ReentrantLock to avoid multiple threads refreshing concurrently
Use CompletableFuture or async call for refresh
Add retry + exponential backoff if external call fails
Use Caffeine with CacheLoader.refreshAfterWrite(...)


Q - Imagine you have a product API used by millions of users. How would you use caching to reduce load on your DB
In a high-traffic product API, I’d identify the most frequently accessed or slow-to-query product data and cache that layer.
For example, top-selling products, recently viewed items, or search results.
I’d use a distributed cache like Hazelcast or Redis, so that all instances of my API can share the same cache layer.
TTL would be used for dynamic data like stock levels (e.g., 5 minutes), and static data like descriptions can be cached longer.

How would you decide what data to cache and what not to cache
We basically cache static data like name,price description as these dont change -- meatdata info ,,
should not cache -- inventory count ,dynamic data like dicount code --ttl based cache -- reviews reating can be cached

Mention Where You Cache
In-memory (Caffeine) for per-instance ultra-fast reads
Distributed cache (Hazelcast/Redis) for shared data across nodes

in our product api -- we will cache the data which is mots
To avoid caching rarely accessed items, we maintain a popularity index (say top 1,000 SKUs/hour) and cache only those
We track product popularity via access logs or Redis counters and cache the top-N most requested products,
 updated every 10 mins using a background job.

Invalidation Startergy
For price and stock, we use cache-aside + short TTL + DB triggers to proactively invalidate product IDs when updates occur.