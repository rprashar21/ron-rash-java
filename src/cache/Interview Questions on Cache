what is caching ??
Caching is a temporary storage layer that stores frequently accessed or expensive-to-fetch data in a fast-access medium like RAM.
The main goals are:

Reduce latency
Decrease load on downstream systems (like databases or APIs)
Improve application throughput and scalability
Real-world use cases:
Netflix: Caching movie metadata or user watch history to avoid DB round-trips
Amazon: Caching product pages or stock availability
Google: Caching autocomplete results in search
Backend APIs: Caching authorization tokens or rate-limit counters



Q What if the redis or our cache  is down
If Redis is down, our cache layer is temporarily unavailable, which increases DB load and latency. To handle this gracefully, we:
Fallback to the DB for reads
Write to DB first and cache second to ensure persistence
Wrap cache calls in circuit breakers and use retries with backoff
Use Prometheus and Grafana to monitor cache health and alert SRE teams


Q Different type of cache
1.write thru cache -- data is written in db and cache synchronoulsy
cons - May slow down system under heavy write load. and hig latency, perforamnce is low
Use case - User profile updates, login sessions

2.write behind cache -- cache is updated and the db is updated asynchronously... low latency ,, good peromance
cons- if cache fails data might be lost ,hard to debug
use case -- logginsystems and analytical systems

3. Write-Around Cache -- db is wrtten first and then cache is updated on a read ,
prevents cache pollution
Use case: Bulk data inserts, infrequently accessed writes.

Q Which Would You Choose for a Financial Transaction System?
For a financial system like payments or fund transfers, I'd choose write-through caching to ensure t
hat every write is persisted to the database immediately.
 This guarantees data consistency and minimizes the risk of loss, even though it comes at a performance cost.

q in memory cache vs distributed cache
data is stored in aplication lvel , cannot survive restanrts,, fast as no netwrok callls
Caching configuration values, recent DB query results, or small reference data in a single-node app
eg linkedlist+hashmap

Distributed cache -- Examples: Redis, Memcached, Hazelcast
cale horizontal
data is shared across intsance , data can be restored survice app restart

Caching authentication tokens, user sessions, search results in microservices


Q Differernce between LRU LFU AND TTL
| Policy | Eviction Logic        | Used For                          |
| ------ | --------------------- | --------------------------------- |
| LRU    | Least Recently Used   | Web sessions, product views       |
| LFU    | Least Frequently Used | Caching most used data (e.g. DNS) |
| TTL    | Time-based Expiry     | OTPs, price cache, auth tokens    |

TTL -- expiry time with caching -- look at the springboot example and also in memory cache examples
Use ReentrantLock to avoid multiple threads refreshing concurrently
Use CompletableFuture or async call for refresh
Add retry + exponential backoff if external call fails
Use Caffeine with CacheLoader.refreshAfterWrite(...)


Q - Imagine you have a product API used by millions of users. How would you use caching to reduce load on your DB
In a high-traffic product API, I‚Äôd identify the most frequently accessed or slow-to-query product data and cache that layer.
For example, top-selling products, recently viewed items, or search results.
I‚Äôd use a distributed cache like Hazelcast or Redis, so that all instances of my API can share the same cache layer.
TTL would be used for dynamic data like stock levels (e.g., 5 minutes), and static data like descriptions can be cached longer.

How would you decide what data to cache and what not to cache
We basically cache static data like name,price description as these dont change -- meatdata info ,,
should not cache -- inventory count ,dynamic data like dicount code --ttl based cache -- reviews reating can be cached

Mention Where You Cache
In-memory (Caffeine) for per-instance ultra-fast reads
Distributed cache (Hazelcast/Redis) for shared data across nodes

in our product api -- we will cache the data which is mots
To avoid caching rarely accessed items, we maintain a popularity index (say top 1,000 SKUs/hour) and cache only those
We track product popularity via access logs or Redis counters and cache the top-N most requested products,
 updated every 10 mins using a background job.

Invalidation Startergy
For price and stock, we use cache-aside + short TTL + DB triggers to proactively invalidate product IDs when updates occur.
Let‚Äôs trace three users:

üë§ User 1 ‚Äì from India (browsing products)
üë§ User 2 ‚Äì from UK (searching)
üë§ User 3 ‚Äì from US (buying a product)

user1 hits the website -- the dns is resolved it smae for all the users -- say https://shop.globalmart.com
Azure Front Door (a Global Load Balancer) uses Anycast routing + health probes to route users to the nearest healthy region.
| User   | Routed To            | RTT (approx) |
| ------ | -------------------- | ------------ |
| User 1 | India Central region | 20 ms        |
| User 2 | UK South             | 15 ms        |
| User 3 | East US 2            | 25 ms        |

Front Door also caches static content (images, CSS, product list JSONs).
If the response is cached ‚Üí served from nearest edge POP instantly.

the users calls -- cdn -- if teh data is cached the respnse is quikc
what do we usually cache here


üß© Step 2: Application Gateway ‚Üí AKS Service
Requests hitting /api/products or /api/cart are routed via
Azure Application Gateway: terminates SSL, routes based on URL path.
/api/proucts/* ‚Üí Product Service
/api/orders/* ‚Üí Order Service

Aks cluster will be running multiple pods of microservice (prodcut api,order api, paymenst)
autoscaling(hpa)-- when there is traffic increase the no of pods
if not a aks solution -- increase or decerease the the instances
Autoscaling (HPA) based on CPU/RPS thresholds.
When traffic surges (e.g., during sales), new pods spin up within ~30 seconds
When idle, pods scale down to save cost

üß© Step 3: Caching in Action (Reduce DB load)
say users are hitting /api/products/1234 ---> check in cache if nt cache will be missed and then db
Hot items (e.g., top 1000 products) are kept permanently warm in cache.

üß© Step 4: Search Requests (User 2)
user2 searchers /api/search?q=shoes. --Front Door CDN may cache popular search queries eg /api/search?q=iphone17 for 2m TTL
Product Search Service queries ElasticSearch or Cosmos DB (index)
results are cached in the redis cache and then CDN layer further reduces traffic from global users.

Step 5: Checkout Flow (User 3 buying)
User clicks ‚ÄúBuy Now‚Äù ‚Üí /api/checkou
Caching not used here (must hit live systems).
API validates inventory from Azure SQL (primary).
Order placed ‚Üí event sent to Azure Service Bus / Event Grid.
Order confirmation stored in DB ‚Üí returns 200 OK.
Event triggers inventory update ‚Üí cache invalidation for that product.

‚úÖ Cache Invalidation:
Redis key product:12345 evicted or updated.
Ensures next product read reflects reduced stock.


Data Layer
Azure SQL (regional DB) for write-heavy operations like orders.
Cosmos DB with multi-master replication for product catalog
Ensures users in India/US/UK get local read copies.
rites (product updates) replicate asynchronously.

Typical api flow
| Flow                 | Caching Used      | Data Source       | Notes                       |
| -------------------- | ----------------- | ----------------- | --------------------------- |
| `/api/products/{id}` | Redis + CDN       | Cosmos DB (read)  | TTL 30m                     |
| `/api/search`        | Redis             | ElasticSearch     | TTL 10m                     |
| `/api/top-products`  | CDN + Redis       | Precomputed store | TTL 1h                      |
| `/api/checkout`      | No cache          | SQL (write)       | Real-time consistency       |
| `/api/price/{id}`    | Redis (short TTL) | SQL               | TTL 5m (frequently updated) |
