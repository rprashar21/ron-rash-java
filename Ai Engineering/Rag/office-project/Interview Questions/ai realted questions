we are build a rag based application --> lingeting leagl documensta nd answerig strictly from these chunks of data


How We Prevent Hallucinations:
Every query is answered strictly from the retrieved chunks.
Our prompt structure clearly says:
‚ÄúOnly answer from the provided context. If unsure, say you don‚Äôt know.‚Äù

We use LangChain4j-based semantic chunking
DocumentSplitter splitter = DocumentSplitters.recursive(config.getChunkSize(), config.getChunkOverlap());
This improves context quality by splitting on logical sentence/semantic boundaries ‚Äî not raw page breaks ‚Äî so retrieval gives more coherent results.

Every chunk is includes pagenumber for ssource citation , idpic_id ,sectionTag --that inculde offecne,previos convictions,defendant statemnets etc
chunk.setSourceDocumentUrl(sasUrl);
chunk.setPageNumber(pageIndex);
chunk.setSemanticContext("Chunk " + (i + 1) + " of " + segments.size());
chunk.setCaseId(idpcId);

This allows per-response auditing, debugging, and future feedback loops.
If a response is wrong, we can trace it back to the chunk/page


You‚Äôve handled embedding failures gracefully
try {
    List<Double> vector = embeddingService.embedQuery(chunk.getText());
    chunk.setEmbedding(vector);
} catch (Exception e) {
    logger.error("Failed to embed chunk on page " + chunk.getPageNumber());
}

Key Components of our Rag Service
Azure Document Intellignce - is amnaged service by azure -- helps to extract text from the pdfs
maintains a proper structure -- we use a custom model where we have trained on sample idpcs -- bsaically labelling the documenst with
witness section,defendants sections handwritten police statemnets --it also use ocr
Langchain4j for semantically chunking  the document using RecursiveCharacterTextSplitter
Azure OpenAI embeddings (via EmbeddingService) to vectorize chunks and store each chunk wit
Basically every chunk stores the details witness_statement,forensic_report
Use re-ranking via BGE-reranker or Azure Cognitive Search reranker to prioritize legally critical chunks

Every chunk is includes pagenumber for source citation , idpic_id ,sectionTag --that inculde offecne,previos convictions,defendant statemnets etc
chunk.setSourceDocumentUrl(sasUrl);
chunk.setPageNumber(pageIndex);
chunk.setSemanticContext("Chunk " + (i + 1) + " of " + segments.size());
chunk.setCaseId(idpcId);
chunk.sectionTag()--like witness statemenst or forensic reports or offences ,previous offences

In short to reduce halluncination
Our system minimizes hallucination by limiting the LLM to semantically chunked content from trusted
IDPC documents using Azure Document Intelligence and LangChain4j.
Each chunk is tagged with rich metadata for traceability and embedded with error protection.
We tune prompts and structure retrieval in a way that encourages grounded, verifiable responses.
We also have chunk-level auditability, and the architecture supports future enhancement for feedback loops and section-aware reasoning

Basically the documnet is chunked woth sections -- each embedding know what section idpc_id id it is under
We introduce a sectionType tag ‚Äî e.g., witness_statement, offence_summary, forensic_report, previous_convictions

so by tagging each chunk with sematic section type we prevent the llm to hallucinate ,stay grounded, be tracebale , enabale feedback loops

when we retireve data --
Retrieval
You‚Äôre filtering by idpcId and embedding-based semantic search (good precision)
ystem Prompt
You‚Äôre dynamically injecting query-specific system prompts using QueryPromptMappingUtil.getSystemPrompt(userQuery)
üîç Retrieval
You‚Äôre filtering by idpcId and embedding-based semantic search (good precision)
üîó LLM Invocation
You‚Äôre passing userQuery + retrievedDocuments + systemPrompt to llmService.getLlmResponse


Q1. Is your RAG system cost-efficient at scale
This tests understanding of token consumption, azure services cost
Document chunking is page-based, ensuring minimal but meaningful units of embedding. Document Intellignece cost is very minimal

Strategies to Ensure Credibility & Reduce Hallucination:
Closed-Context Retrieval
 We allow the llm to generate response only on the retrieved chunks of the document- which are retrieved via the azure ai search on idpcs
 For every question there are specific promts -- which makes sure if no infirmaton is gathered it will not probide a respinse

 2. Page level chunking and citation --sectionTag

 3. System Prompt Tuning:
 4. Chunk Quality Control: - Prioritize high-signal content like witness statements, offense descriptions
 5. Bias Mitigation via Prompting
 To reduce bias (e.g., gendered, racial), we:
 Normalize names (John Doe / Jane Doe style)
 Remove unnecessary demographic info unless legally relevant
 Reinforce neutrality in the prompt:
 ‚ÄúPresent the facts only, avoid opinion or speculation.

 In short
 Retrieve only from indexed legal documents
 Limit LLM to strict context
 Prompt for neutral, fact-based responses
 Cite sources in output


| Component                               | What It Does                                   | Pricing                               | Daily Volume     | Est. Daily Cost |
| --------------------------------------- | ---------------------------------------------- | ------------------------------------- | ---------------- | --------------- |
| **üìÑ Azure Document Intelligence**      | Parses & extracts raw text & structure         | \~**\$10 per 1,000 pages** (Read API) | \~1M pages       | **\$10,000** ‚ö†Ô∏è |
| **üî§ Azure OpenAI Embedding**           | Converts text to vectors (`embedding-3-small`) | \~\$0.00002 / 1K tokens               | \~1.5B tokens    | \~\$30          |
| **üß† LLM Querying (e.g. GPT-35-Turbo)** | Answers user queries                           | \~\$0.003‚Äì\$0.004 per query           | 20K queries      | \~\$60‚Äì80       |
| **üì¶ Blob Storage (Hot Tier)**          | Stores original docs                           | \$0.0184 / GB                         | \~30 GB          | \~\$0.55        |
| **üîç Azure AI Search (Vector Index)**   | Vector search + storage                        | \~\$0.40/GB/month                     | \~1‚Äì2GB/day      | \~\$20‚Äì30/month |
| **‚öôÔ∏è Azure Functions (Event-Driven)**   | Ingestion + processing                         | \~\$0.20 per 1M execs                 | \~500K execs/day | \~\$0.10        |
| **üì© Storage Queues**                   | Queue ingestion flow                           | \~\$0.01 per million ops              | \~500K ops/day   | \~\$0.01        |
| **üìä Monitoring & Logs**                | Azure Monitor/App Insights                     | Variable                              | ‚Äî                | \~\$2‚Äì5/day     |


